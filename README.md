An Hour of Learning: Deep Learning for NLP

**1. MLPs and Embeddings: Word2Vec, GloVe, fastText** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp1/?referralCode=60FD271AEA691F58263F)
- Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations by back-propagating errors." nature 323, no. 6088 (1986): 533-536.
- Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. "Distributed representations of words and phrases and their compositionality." In Advances in neural information processing systems, pp. 3111-3119. 2013.
- Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543.
- Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. "Fasttext. zip: Compressing text classification models." arXiv preprint arXiv:1612.03651 (2016).
- Marvin, Minsky, and A. Papert Seymour. "Perceptrons." (1969).
- http://playground.tensorflow.org/ 
- Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. "A neural probabilistic language model." journal of machine learning research 3, no. Feb (2003): 1137-1155.
- Morin, Frederic, and Yoshua Bengio. "Hierarchical probabilistic neural network language model." In Aistats, vol. 5, pp. 246-252. 2005.
- Chen, Welin, David Grangier, and Michael Auli. "Strategies for training large vocabulary neural language models." arXiv preprint arXiv:1512.04906 (2015).
- Jozefowicz, Rafal, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. "Exploring the limits of language modeling." arXiv preprint arXiv:1602.02410 (2016).
- Bengio, Yoshua, and Jean-Sébastien Senécal. "Adaptive importance sampling to accelerate training of a neural probabilistic language model." IEEE Transactions on Neural Networks 19, no. 4 (2008): 713-722.
- Mnih, A., & Teh, Y. W. (2012). A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. Proceedings of the 29th International Conference on Machine Learning (ICML’12), 1751–1758. 
- Ruder, Sebastian, Ivan Vulić, and Anders Søgaard. "A survey of cross-lingual word embedding models." Journal of Artificial Intelligence Research 65 (2019): 569-631.
- Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural machine translation of rare words with subword units." arXiv preprint arXiv:1508.07909 (2015).
- Schuster, Mike, and Kaisuke Nakajima. "Japanese and korean voice search." In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5149-5152. IEEE, 2012.
- Kudo, Taku, and John Richardson. "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing." arXiv preprint arXiv:1808.06226 (2018).

**2. RNNs, GRUs, LSTMs, variants.** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp1/?referralCode=60FD271AEA691F58263F)
- https://colah.github.io/posts/2015-08-Understanding-LSTMs/
- http://karpathy.github.io/2015/05/21/rnn-effectiveness/
- Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9, no. 8 (1997): 1735-1780.
- Schuster, Mike, and Kuldip K. Paliwal. "Bidirectional recurrent neural networks." IEEE transactions on Signal Processing 45, no. 11 (1997): 2673-2681.
- https://en.wikipedia.org/wiki/Recurrent_neural_network
- Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. "Learning phrase representations using RNN encoder-decoder for statistical machine translation." arXiv preprint arXiv:1406.1078 (2014).


**3. Encoder-decoder, attention models, ELMo**  [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp2/?referralCode=3BD3627FAE3FF1F5CDC9)
 
- Cho, Kyunghyun, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. "Learning phrase representations using RNN encoder-decoder for statistical machine translation." arXiv preprint arXiv:1406.1078 (2014). https://arxiv.org/pdf/1406.1078.pdf
- Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." arXiv preprint arXiv:1409.0473 (2014). https://arxiv.org/pdf/1409.0473.pdf
- Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. "Deep contextualized word representations." arXiv preprint arXiv:1802.05365 (2018).
- Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "Sequence to sequence learning with neural networks." arXiv preprint arXiv:1409.3215 (2014). https://arxiv.org/pdf/1409.3215
- Effective Approaches to Attention-based Neural Machine Translation, 2015. Luong15_emnlp
- Yang, Zichao, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. "Hierarchical attention networks for document classification." NAACL, pp. 1480-1489. 2016.
- Yin, Wenpeng, Hinrich Schütze, Bing Xiang, and Bowen Zhou. "Abcnn: Attention-based convolutional neural network for modeling sentence pairs." Transactions of the Association for Computational Linguistics 4 (2016): 259-272.
- Rocktäschel, Tim, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiský, and Phil Blunsom. "Reasoning about entailment with neural attention." arXiv preprint arXiv:1509.06664 (2015).

**4. Transformers, GPT, GPT2, GPT3, BERT, GLUE** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp2/?referralCode=3BD3627FAE3FF1F5CDC9) 
- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need." In NIPS, pp. 5998-6008. 2017. 
- Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. "Improving language understanding by generative pre-training." (2018).
- McCann, Bryan, James Bradbury, Caiming Xiong, and Richard Socher. "Learned in translation: Contextualized word vectors." In Advances in Neural Information Processing Systems, pp. 6294-6305. 2017.
- Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." NAACL, pp. 4171-4186. 2019.
- https://gluebenchmark.com/
- http://jalammar.github.io/
- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
- https://github.com/google-research/bert/blob/master/multilingual.md
- Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. "Language models are unsupervised multitask learners." OpenAI blog 1, no. 8 (2019): 9.

**5. Sentence Embeddings: Doc2Vec, USE, InferSent, SentenceBERT** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp3/?referralCode=A72B2CDC6B0306FB80E4)
- http://proceedings.mlr.press/v37/kusnerb15.pdf
- Arora, Sanjeev, Yingyu Liang, and Tengyu Ma. "A simple but tough-to-beat baseline for sentence embeddings." In 5th International Conference on Learning Representations, ICLR 2017. 2017.
- Le, Quoc, and Tomas Mikolov. "Distributed representations of sentences and documents." In International conference on machine learning, pp. 1188-1196. PMLR, 2014.
- Kiros, Ryan, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. "Skip-thought vectors." arXiv preprint arXiv:1506.06726 (2015).
- Logeswaran, Lajanugen, and Honglak Lee. "An efficient framework for learning sentence representations." In International Conference on Learning Representations. 2018.
- Socher et al. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. ICML 2011
- Iyyer, Mohit, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daumé III. "Deep unordered composition rivals syntactic methods for text classification." ACL, pp. 1681-1691. 2015.
- Conneau, Alexis, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. "Supervised learning of universal sentence representations from natural language inference data." arXiv preprint arXiv:1705.02364 (2017).
- Shen, Yelong, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. "Learning semantic representations using convolutional neural networks for web search." In Proceedings of the 23rd international conference on world wide web, pp. 373-374. 2014.
- Cer, Daniel, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant et al. "Universal sentence encoder." arXiv preprint arXiv:1803.11175 (2018).
- Liu, Xiaodong, Pengcheng He, Weizhu Chen, and Jianfeng Gao. "Multi-task deep neural networks for natural language understanding." arXiv preprint arXiv:1901.11504 (2019).
- Subramanian, Sandeep, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. "Learning general purpose distributed sentence representations via large scale multi-task learning." arXiv preprint arXiv:1804.00079 (2018).
- Reimers, Nils, and Iryna Gurevych. "Sentence-bert: Sentence embeddings using siamese bert-networks." arXiv preprint arXiv:1908.10084 (2019).

**6. Generative Transformer models: Transformer-XL, XLNet, ProphetNet, UniLM, MASS, CTRL, T5, BART** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp3/?referralCode=A72B2CDC6B0306FB80E4)
- Dong, Li, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. "Unified language model pre-training for natural language understanding and generation." In Advances in Neural Information Processing Systems, pp. 13063-13075. 2019.
- Dai, Zihang, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. "Transformer-xl: Attentive language models beyond a fixed-length context." arXiv:1901.02860 (2019).
- Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. "XLNet: Generalized Autoregressive Pretraining for Language Understanding." arXiv preprint arXiv:1906.08237 (2019).
- Song, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. "Mass: Masked sequence to sequence pre-training for language generation." arXiv preprint arXiv:1905.02450 (2019).
- Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension." arXiv preprint arXiv:1910.13461 (2019).
- Keskar, Nitish Shirish, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. "Ctrl: A conditional transformer language model for controllable generation." arXiv preprint arXiv:1909.05858 (2019).
- Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. "Exploring the limits of transfer learning with a unified text-to-text transformer." arXiv preprint arXiv:1910.10683 (2019).
- Yan, Yu, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. "Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training." arXiv preprint arXiv:2001.04063 (2020).

**7. Multi/Cross-lingual NLP: Part 1** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp4/?referralCode=1C6EDCED5AA51177725B)
- XNLI: Conneau, Alexis, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. "XNLI: Evaluating cross-lingual sentence representations." arXiv preprint arXiv:1809.05053 (2018).
- mBERT: Pires, Telmo, Eva Schlinger, and Dan Garrette. "How Multilingual is Multilingual BERT?." ACL, pp. 4996-5001. 2019.
- XLM: Conneau, Alexis, and Guillaume Lample. "Cross-lingual Language Model Pretraining." NIPS, pp. 7057-7067. 2019.
- xQuAD: Artetxe, Mikel, Sebastian Ruder, and Dani Yogatama. "On the cross-lingual transferability of monolingual representations." arXiv preprint arXiv:1910.11856 (2019).
- Unicoder: Huang, Haoyang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. "Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks." EMNLP-IJCNLP, pp. 2485-2494. 2019.
- XLM-R: Conneau, Alexis, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. "Unsupervised cross-lingual representation learning at scale." ACL 2020.
- XGLUE: Liang, Yaobo, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong et al. "Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation." arXiv preprint arXiv:2004.01401 (2020).

**8. Multi/Cross-lingual NLP: Part 2** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp4/?referralCode=1C6EDCED5AA51177725B)
- XNLG: Chi, Zewen, Li Dong, Furu Wei, Wenhui Wang, Xian-Ling Mao, and Heyan Huang. "Cross-Lingual Natural Language Generation via Pre-Training." In AAAI, pp. 7570-7577. 2020. https://arxiv.org/pdf/1909.10481.pdf 
- XTREME: Hu, Junjie, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation." In International Conference on Machine Learning, pp. 4411-4421. PMLR, 2020.
- XTREME-R: Ruder, Sebastian, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, and Melvin Johnson. "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation." arXiv preprint arXiv:2104.07412 (2021).
- mBART: Liu, Yinhan, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. "Multilingual denoising pre-training for neural machine translation." arXiv preprint arXiv:2001.08210 (2020).
- InfoXLM: Chi, Zewen, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training." arXiv preprint arXiv:2007.07834 (2020).
- FILTER: Fang, Yuwei, Shuohang Wang, Zhe Gan, Siqi Sun, and Jingjing Liu. "FILTER: An enhanced fusion method for cross-lingual language understanding." arXiv preprint arXiv:2009.05166 (2020).
- mT5: Xue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. "mT5: A massively multilingual pre-trained text-to-text transformer." arXiv preprint arXiv:2010.11934 (2020).

**9. Efficient Transformers: Part 1** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp5/?referralCode=BB55A5E37EDA4BF911D8)
- Guo, Qipeng, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. "Star-transformer." arXiv preprint arXiv:1902.09113 (2019).
- Child, Rewon, Scott Gray, Alec Radford, and Ilya Sutskever. "Generating long sequences with sparse transformers." arXiv preprint arXiv:1904.10509 (2019).
- Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. "Reformer: The efficient transformer." arXiv preprint arXiv:2001.04451 (2020).
- Beltagy, Iz, Matthew E. Peters, and Arman Cohan. "Longformer: The long-document transformer." arXiv preprint arXiv:2004.05150 (2020).
- Tay, Yi, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. "Synthesizer: Rethinking self-attention in transformer models." arXiv preprint arXiv:2005.00743 (2020).
- Wang, Sinong, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. "Linformer: Self-attention with linear complexity." arXiv preprint arXiv:2006.04768 (2020).

**10. Efficient Transformers: Part 2** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp5/?referralCode=BB55A5E37EDA4BF911D8)
- Tay, Yi, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. "Sparse sinkhorn attention." In International Conference on Machine Learning, pp. 9438-9447. PMLR, 2020.
- Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins et al. "Rethinking attention with performers." arXiv preprint arXiv:2009.14794 (2020).
- Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. "Transformers are rnns: Fast autoregressive transformers with linear attention." In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020.
- Zaheer, Manzil, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham et al. "Big bird: Transformers for longer sequences." arXiv preprint arXiv:2007.14062 (2020).
- Roy, Aurko, Mohammad Saffar, Ashish Vaswani, and David Grangier. "Efficient content-based sparse attention with routing transformers." Transactions of the Association for Computational Linguistics 9 (2021): 53-68.
- Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. "Efficient transformers: A survey." arXiv preprint arXiv:2009.06732 (2020).

**11. Some popular Transformer Models** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp6/?referralCode=2B19F7309215C76C4D88)
- Zhang, Yizhe, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. "Dialogpt: Large-scale generative pre-training for conversational response generation." arXiv preprint arXiv:1911.00536 (2019).
- Li, Liunian Harold, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. "Visualbert: A simple and performant baseline for vision and language." arXiv preprint arXiv:1908.03557 (2019).
- Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks." arXiv preprint arXiv:1908.02265 (2019).
- Clark, Kevin, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. "Electra: Pre-training text encoders as discriminators rather than generators." arXiv preprint arXiv:2003.10555 (2020).
- He, Pengcheng, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. "Deberta: Decoding-enhanced bert with disentangled attention." arXiv preprint arXiv:2006.03654 (2020).
- Joshi, Mandar, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. "Spanbert: Improving pre-training by representing and predicting spans." Transactions of the Association for Computational Linguistics 8 (2020): 64-77.
- Fedus, William, Barret Zoph, and Noam Shazeer. "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity." arXiv preprint arXiv:2101.03961 (2021).
- Lepikhin, Dmitry, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. "Gshard: Scaling giant models with conditional computation and automatic sharding." arXiv preprint arXiv:2006.16668 (2020).

**12. Model Compression for NLP: Pruning, Quantization** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp7/?referralCode=A10EED43FFCAB8C27139)
- Gupta, Manish, and Puneet Agrawal. "Compression of Deep Learning Models for Text: A Survey." arXiv preprint arXiv:2008.05221 (2020).

**13. Model Compression for NLP: Knowledge Distillation** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp7/?referralCode=A10EED43FFCAB8C27139)
- Gupta, Manish, and Puneet Agrawal. "Compression of Deep Learning Models for Text: A Survey." arXiv preprint arXiv:2008.05221 (2020).

**14. Graph neural networks** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp8/?referralCode=9A526212A04A819625A3)
- Graph Neural Networks: Models and Applications. Yao Ma, Wei Jin, Jiliang Tang, Yiqi Wang, Tyler Derr. AAAI 2021 tutorial.
- A Tutorial on Graph Neural Networks for Natural Language Processing. Shikhar Vashishth, Y. Naganand, Partha Talukdar. EMNLP 2019 tutorial.
- Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016).
- Veličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. "Graph attention networks." arXiv preprint arXiv:1710.10903 (2017).
- Vashishth, Shikhar, Prateek Yadav, Manik Bhandari, and Partha Talukdar. "Confidence-based graph convolutional networks for semi-supervised learning." In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1792-1801. PMLR, 2019.
- Marcheggiani, Diego, and Ivan Titov. "Encoding sentences with graph convolutional networks for semantic role labeling." arXiv preprint arXiv:1703.04826 (2017).
- Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. "Neural message passing for quantum chemistry." In International conference on machine learning, pp. 1263-1272. PMLR, 2017.
- Dhillon, Inderjit S., Yuqiang Guan, and Brian Kulis. "Weighted graph cuts without eigenvectors a multilevel approach." IEEE transactions on pattern analysis and machine intelligence 29, no. 11 (2007): 1944-1957.
- Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. "Order matters: Sequence to sequence for sets." arXiv preprint arXiv:1511.06391 (2015).
- Zhang, Muhan, Zhicheng Cui, Marion Neumann, and Yixin Chen. "An end-to-end deep learning architecture for graph classification." In Thirty-Second AAAI Conference on Artificial Intelligence. 2018.
- Ying, Rex, Jiaxuan You, Christopher Morris, Xiang Ren, William L. Hamilton, and Jure Leskovec. "Hierarchical graph representation learning with differentiable pooling." arXiv preprint arXiv:1806.08804 (2018).
- Gao, Hongyang, and Shuiwang Ji. "Graph u-nets." In international conference on machine learning, pp. 2083-2092. PMLR, 2019.
- Cangea, Cătălina, Petar Veličković, Nikola Jovanović, Thomas Kipf, and Pietro Liò. "Towards sparse hierarchical graph classifiers." arXiv preprint arXiv:1811.01287 (2018).
- Lee, Junhyun, Inyeop Lee, and Jaewoo Kang. "Self-attention graph pooling." In International Conference on Machine Learning, pp. 3734-3743. PMLR, 2019.
- Hamilton, William L., Rex Ying, and Jure Leskovec. "Inductive representation learning on large graphs." In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025-1035. 2017.
- Bastings, Jasmijn, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. "Graph convolutional encoders for syntax-aware neural machine translation." arXiv preprint arXiv:1704.04675 (2017). https://aclweb.org/anthology/papers/D/D17/D17-1209/ 
- Beck, Daniel, Gholamreza Haffari, and Trevor Cohn. "Graph-to-sequence learning using gated graph neural networks." arXiv preprint arXiv:1806.09835 (2018).
- Nguyen, Thien Huu, and Ralph Grishman. "Graph convolutional networks with argument-aware pooling for event detection." In Thirty-second AAAI conference on artificial intelligence. 2018.
- Liu, Xiao, Zhunchen Luo, and Heyan Huang. "Jointly multiple events extraction via attention-based graph information aggregation." arXiv preprint arXiv:1809.09078 (2018).
- Vashishth, Shikhar, Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. "Dating documents using graph convolution networks." arXiv preprint arXiv:1902.00175 (2019).
- Vashishth, Shikhar, Rishabh Joshi, Sai Suman Prayaga, Chiranjib Bhattacharyya, and Partha Talukdar. "Reside: Improving distantly-supervised neural relation extraction using side information." arXiv preprint arXiv:1812.04361 (2018).

**15. Hate speech detection** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp9/?referralCode=37FE62AC40DA236073C8)
- Badjatiya, Pinkesh,  Gupta,  S.,Gupta, Manish, Varma, Vasudeva:  Deep  learning  for hate speech detection in tweets. In: Proceedings of the 26th international conference on World Wide Web companion. pp. 759–760 (2017)
- Bhardwaj, M., Akhtar, M.S., Ekbal, A.,Das, Amitava, Chakraborty, Tanmoy: Hostility detection dataset in hindi. arXiv preprint arXiv:2011.03588 (2020)
- Burnap, P., Williams, M.L.: Us and them: identifying cyber hate on twitter across multiple protected characteristics. EPJ Data science5, 1–15 (2016)
- Chandra, M., Pathak, A., Dutta, E., Jain, P.,Gupta, Manish, Shrivastava, M., Kumaraguru,P.: Abuseanalyzer: Abuse detection, severity and target prediction for gab posts. In: Proceedings of the 28th International Conference on Computational Linguistics. pp. 6277–6283 (2020)
- Cheng, L., Shu, K., Wu, S., Silva, Y.N., Hall, D.L., Liu, H.: Unsupervised cyberbullying detection via time-informed gaussian mixture model. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. pp. 185–194 (2020)
- Das,  A.,  Wahi,  J.S.,  Li,  S.:  Detecting  hate  speech  in  multi-modal  memes.  arXiv  preprint arXiv:2012.14891 (2020)
- Davidson, T., Warmsley, D., Macy, M., Weber, I.: Automated hate speech detection and the problem of offensive language. In: Proceedings of the International AAAI Conference on Web and Social Media. vol. 11 (2017)
- Fortuna, P., Nunes, S.: A survey on automatic detection of hate speech in text. ACM Computing Surveys (CSUR)51(4), 1–30 (2018)
- Founta, A.M., Chatzakou, D., Kourtellis, N., Blackburn, J., Vakali, A., Leontiadis, I.: A unified deep learning architecture for abuse detection. In: Proceedings of the 10th ACM conference on web science. pp. 105–114 (2019)
- Gomez,  R.,  Gibert,  J.,  Gomez,  L.,  Karatzas,  D.:  Exploring  hate  speech  detection  in  multi-modal publications. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1470–1478 (2020)
- Juuti, M., Gr ̈ondahl, T., Flanagan, A., Asokan, N.: A little goes a long way: Improving toxic language classification despite data scarcity. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. pp. 2991–3009 (2020)
- Karlekar, S., Bansal, M.: Safecity: Understanding diverse forms of sexual harassment personal stories.  In:  Proceedings  of  the  2018  Conference  on  Empirical  Methods  in  Natural  Language Processing. pp. 2805–2811 (2018)
- Kennedy, B., Jin, X., Davani, A.M., Dehghani, M., Ren, X.: Contextualizing hate speech classifiers with post-hoc explanation. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5435–5442 (2020)
- Kiela, D., Firooz, H., Mohan, A., Goswami, V., Singh, A., Ringshia, P., Testuggine, D.: The hateful  memes  challenge:  Detecting  hate  speech  in  multimodal  memes.  Advances  in  Neural Information Processing Systems33(2020)
- MacAvaney, S., Yao, H.R., Yang, E., Russell, K., Goharian, N., Frieder, O.: Hate speech detection: Challenges and solutions. PloS one14(8), e0221152 (2019)
- Mou,  G.,  Ye,  P.,  Lee,  K.:  Swe2:  Subword  enriched  and  significant  word  emphasized  frame-work for hate speech detection. In: Proceedings of the 29th ACM International Conference on Information & Knowledge Management. pp. 1145–1154 (2020)
- Nobata, C., Tetreault, J., Thomas, A., Mehdad, Y., Chang, Y.: Abusive language detection in online user content. In: Proceedings of the 25th international conference on world wide web. pp. 145–153 (2016)
- Parikh, P., Abburi, H.,Badjatiya, Pinkesh, Krishnan, R., Chhaya, N.,Gupta, M.,Varma, Vasudeva:  Multi-label  categorization  of  accounts  of  sexism  using  a  neural  framework.  In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).pp. 1642–1652 (2019)
- Parikh, P., Abburi, H., Chhaya, N.,Gupta, Manish, Varma, Vasudeva: Categorizing sexism and misogyny through neural approaches. Transactions of the Web (TWEB) (2021)
- Qian, J., ElSherief, M., Belding, E., Wang, W.Y.: Hierarchical cvae for fine-grained hate speechclassification. arXiv preprint arXiv:1809.00088 (2018)
- Sharma, C., Bhageria, D., Scott, W., PYKL, S.,Das, Amitava, Chakraborty, Tanmoy, Pulabaigari,  V.,  Gamback,  B.:  Semeval-2020  task  8:  Memotion  analysis–the  visuo-lingual metaphor! arXiv preprint arXiv:2008.03781 (2020)
- Silva, L., Mondal, M., Correa, D., Benevenuto, F., Weber, I.: Analyzing the targets of hate in online social media. In: Proceedings of the International AAAI Conference on Web and Social Media. vol. 10 (2016)
- Suvarna, A., Bhalla, G.: # notawhore! a computational linguistic perspective of rape culture and victimization on social media. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. pp. 328–335 (2020)
- Waseem, Z.: Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter. In: Proceedings of the first workshop on NLP and computational social science. pp.138–142 (2016)
- Waseem, Z., Hovy, D.: Hateful symbols or hateful people? predictive features for hate speech detection  on  twitter.  In:  Proceedings  of  the  NAACL  student  research  workshop.  pp.  88–93(2016)
- Yang, F., Peng, X., Ghosh, G., Shilon, R., Ma, H., Moore, E., Predovic, G.: Exploring deep multimodal fusion of text and photo for hate speech classification. In: Proceedings of the Third Workshop on Abusive Language Online. pp. 11–18 (2019)
- Zhang, Z., Luo, L.: Hate speech detection: A solved problem? the challenging case of long tail on twitter. Semantic Web10(5), 925–945 (2019)
- Zhong, H., Li, H., Squicciarini, A.C., Rajtmajer, S.M., Griffin, C., Miller, D.J., Caragea, C.:Content-driven detection of cyberbullying on the instagram social network. In: IJCAI. vol. 16,pp. 3952–3958 (2016)

**16. Fake news detection** [Udemy Course Link](https://www.udemy.com/course/ahol-dl4nlp10/?referralCode=C3284FAEE06D6AD036C7)
- Fake News Research: Theories, Detection Strategies, and Open Problems. Reza Zafarani, Xinyi Zhou, Kai Shu, Huan Liu. Tutorial | 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2019.
- Popat, Kashyap, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. "Declare: Debunking fake news and false claims using evidence-aware deep learning." arXiv preprint arXiv:1809.06416 (2018).
- Valerie Hauch, Iris Blandón-Gitlin, Jaume Masip, and Siegfried L Sporer. 2015. Are computers effective lie detectors? A meta-analysis of linguistic cues to deception. Personality and Social Psychology Review 19, 4 (2015), 307–342.
- Zhou, Xinyi, Atishay Jain, Vir V. Phoha, and Reza Zafarani. "Fake news early detection: A theory-driven model." Digital Threats: Research and Practice 1, no. 2 (2020): 1-25.
- Wang, William Yang. "Liar, liar pants on fire": A new benchmark dataset for fake news detection. arXiv preprint arXiv:1705.00648 (2017).
- Castillo, Carlos, Marcelo Mendoza, and Barbara Poblete. "Information credibility on twitter." In Proceedings of the 20th international conference on World wide web, pp. 675-684. 2011.
- Yang, Yang, Lei Zheng, Jiawei Zhang, Qingcai Cui, Zhoujun Li, and Philip S. Yu. "TI-CNN: Convolutional neural networks for fake news detection." arXiv preprint arXiv:1806.00749 (2018).
- Wang, Yaqing, Fenglong Ma, Zhiwei Jin, Ye Yuan, Guangxu Xun, Kishlay Jha, Lu Su, and Jing Gao. "Eann: Event adversarial neural networks for multi-modal fake news detection." KDD, pp. 849-857. 2018.
- Dhruv Khattar, Jaipal Singh Goud, Manish Gupta, Vasudeva Varma. MVAE: Multimodal Variational Autoencoder for Fake News Detection [Poster]. The Web Conference 2019. May 13-17, 2019. San Francisco.
- Huh, Minyoung, Andrew Liu, Andrew Owens, and Alexei A. Efros. "Fighting fake news: Image splice detection via learned self-consistency." In ECCV, pp. 101-117. 2018.
- Ma, Jing, Wei Gao, Prasenjit Mitra, Sejeong Kwon, Bernard J. Jansen, Kam-Fai Wong, and Meeyoung Cha. "Detecting rumors from microblogs with recurrent neural networks." (2016): 3818.
- Ma, Jing, Wei Gao, and Kam-Fai Wong. "Detect rumors in microblog posts using propagation structure via kernel learning." Association for Computational Linguistics, 2017.
- Liu, Yang, and Yi-Fang Brook Wu. "Early detection of fake news on social media through propagation path classification with recurrent and convolutional networks." AAAI 2018.
- Ma, Jing, Wei Gao, and Kam-Fai Wong. "Rumor detection on twitter with tree-structured recursive neural networks." Association for Computational Linguistics, 2018.
- Pinnaparaju, Nikhil, Manish Gupta, and Vasudeva Varma. "T3N: Harnessing Text and Temporal Tree Network for Rumor Detection on Twitter." In PAKDD, pp. 686-700. Springer, Cham, 2021.
- Z. Jin, et al. News Verification by Exploiting Conflicting Social Viewpoints in Microblogs. AAAI’16
- M. Gupta, et al. Evaluating Event Credibility on Twitter. SDM’12
- Shu, Kai, Suhang Wang, and Huan Liu. "Beyond news contents: The role of social context for fake news detection." In Proceedings of the twelfth ACM international conference on web search and data mining, pp. 312-320. 2019.
- Zhang, Jiawei, Bowen Dong, and Philip S. Yu. "FAKEDETECTOR: Effective Fake News Detection with Deep Diffusive Neural Network." (2018). ICDE
- Monti, Federico, Fabrizio Frasca, Davide Eynard, Damon Mannion, and Michael M. Bronstein. "Fake news detection on social media using geometric deep learning." arXiv preprint arXiv:1902.06673 (2019).
- Shu, Kai, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu. "dEFEND: Explainable Fake News Detection." (2019). KDD
- Zellers, Rowan, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. "Defending against neural fake news." arXiv preprint arXiv:1905.12616 (2019).
